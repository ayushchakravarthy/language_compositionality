Namespace(adv_lambda=0.01, adv_lr=0.01, adv_theta=0.01, batch_size=256, cat_xm=False, checkpoint_every=4, checkpoint_path='../weights/language_parser/scan/defaults_addjump.pt', d_model=256, dataset='scan', dim_feedforward=1024, dropout=0.1, filter=256, learning_rate=4e-05, load_weights_from=None, model_type='language_parser', n_layers=6, nhead=8, num_epochs=150, num_runs=1, out_attn_wts='train_defaults_jump_attn_maps', out_data_file='train_defaults_jump', pos=False, record_loss_every=20, results_dir='language_parser', split='addjump', use_adversary=False, use_xv=False)
Model size: 40992266
LanguageParser(
  (src_embedding): Embedding(17, 256)
  (trg_embedding): Embedding(10, 256)
  (positional_encoding): PositionalEncoding(
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (encoder): LPEncoder(
    (act): GELU()
    (layer_0): Stage(
      (blocks): ModuleList(
        (0): LPBlock(
          (encoder): LPLayerEncoder(
            (enc_attn): AnyAttention(
              (norm_q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_k): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_v): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (to_q): Linear(in_features=256, out_features=256, bias=False)
              (to_k): Linear(in_features=256, out_features=256, bias=False)
              (to_v): Linear(in_features=256, out_features=256, bias=False)
              (proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (reason): SimpleReasoning(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear): Conv1d(64, 64, kernel_size=(1,), stride=(1,), bias=False)
            )
            (enc_ffn): MLP(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (fc1): Linear(in_features=256, out_features=256, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=256, out_features=256, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (decoder): LPLayerDecoder(
            (attn1): AnyAttention(
              (norm_q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_k): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_v): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (to_q): Linear(in_features=256, out_features=256, bias=False)
              (to_k): Linear(in_features=256, out_features=256, bias=False)
              (to_v): Linear(in_features=256, out_features=256, bias=False)
              (proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (attn2): AnyAttention(
              (norm_q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_k): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_v): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (to_q): Linear(in_features=256, out_features=256, bias=False)
              (to_k): Linear(in_features=256, out_features=256, bias=False)
              (to_v): Linear(in_features=256, out_features=256, bias=False)
              (proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (ffn1): MLP(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (fc1): Linear(in_features=256, out_features=768, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=768, out_features=256, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ffn2): MLP(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (fc1): Linear(in_features=256, out_features=768, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=768, out_features=256, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (whole_qpos): PositionalEncoding(
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (1): LPBlock(
          (encoder): LPLayerEncoder(
            (enc_attn): AnyAttention(
              (norm_q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_k): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_v): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (to_q): Linear(in_features=256, out_features=256, bias=False)
              (to_k): Linear(in_features=256, out_features=256, bias=False)
              (to_v): Linear(in_features=256, out_features=256, bias=False)
              (proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (reason): SimpleReasoning(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear): Conv1d(64, 64, kernel_size=(1,), stride=(1,), bias=False)
            )
            (enc_ffn): MLP(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (fc1): Linear(in_features=256, out_features=256, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=256, out_features=256, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (decoder): LPLayerDecoder(
            (attn1): AnyAttention(
              (norm_q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_k): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_v): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (to_q): Linear(in_features=256, out_features=256, bias=False)
              (to_k): Linear(in_features=256, out_features=256, bias=False)
              (to_v): Linear(in_features=256, out_features=256, bias=False)
              (proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (attn2): AnyAttention(
              (norm_q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_k): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_v): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (to_q): Linear(in_features=256, out_features=256, bias=False)
              (to_k): Linear(in_features=256, out_features=256, bias=False)
              (to_v): Linear(in_features=256, out_features=256, bias=False)
              (proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (ffn1): MLP(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (fc1): Linear(in_features=256, out_features=768, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=768, out_features=256, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ffn2): MLP(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (fc1): Linear(in_features=256, out_features=768, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=768, out_features=256, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (whole_qpos): PositionalEncoding(
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (2): LPBlock(
          (encoder): LPLayerEncoder(
            (enc_attn): AnyAttention(
              (norm_q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_k): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_v): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (to_q): Linear(in_features=256, out_features=256, bias=False)
              (to_k): Linear(in_features=256, out_features=256, bias=False)
              (to_v): Linear(in_features=256, out_features=256, bias=False)
              (proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (reason): SimpleReasoning(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear): Conv1d(64, 64, kernel_size=(1,), stride=(1,), bias=False)
            )
            (enc_ffn): MLP(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (fc1): Linear(in_features=256, out_features=256, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=256, out_features=256, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (decoder): LPLayerDecoder(
            (attn1): AnyAttention(
              (norm_q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_k): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_v): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (to_q): Linear(in_features=256, out_features=256, bias=False)
              (to_k): Linear(in_features=256, out_features=256, bias=False)
              (to_v): Linear(in_features=256, out_features=256, bias=False)
              (proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (attn2): AnyAttention(
              (norm_q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_k): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_v): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (to_q): Linear(in_features=256, out_features=256, bias=False)
              (to_k): Linear(in_features=256, out_features=256, bias=False)
              (to_v): Linear(in_features=256, out_features=256, bias=False)
              (proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (ffn1): MLP(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (fc1): Linear(in_features=256, out_features=768, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=768, out_features=256, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ffn2): MLP(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (fc1): Linear(in_features=256, out_features=768, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=768, out_features=256, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (whole_qpos): PositionalEncoding(
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (3): LPBlock(
          (encoder): LPLayerEncoder(
            (enc_attn): AnyAttention(
              (norm_q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_k): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_v): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (to_q): Linear(in_features=256, out_features=256, bias=False)
              (to_k): Linear(in_features=256, out_features=256, bias=False)
              (to_v): Linear(in_features=256, out_features=256, bias=False)
              (proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (reason): SimpleReasoning(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear): Conv1d(64, 64, kernel_size=(1,), stride=(1,), bias=False)
            )
            (enc_ffn): MLP(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (fc1): Linear(in_features=256, out_features=256, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=256, out_features=256, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (decoder): LPLayerDecoder(
            (attn1): AnyAttention(
              (norm_q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_k): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_v): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (to_q): Linear(in_features=256, out_features=256, bias=False)
              (to_k): Linear(in_features=256, out_features=256, bias=False)
              (to_v): Linear(in_features=256, out_features=256, bias=False)
              (proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (attn2): AnyAttention(
              (norm_q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_k): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_v): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (to_q): Linear(in_features=256, out_features=256, bias=False)
              (to_k): Linear(in_features=256, out_features=256, bias=False)
              (to_v): Linear(in_features=256, out_features=256, bias=False)
              (proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (ffn1): MLP(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (fc1): Linear(in_features=256, out_features=768, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=768, out_features=256, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ffn2): MLP(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (fc1): Linear(in_features=256, out_features=768, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=768, out_features=256, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (whole_qpos): PositionalEncoding(
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (layer_1): Stage(
      (blocks): ModuleList(
        (0): LPBlock(
          (encoder): LPLayerEncoder(
            (enc_attn): AnyAttention(
              (norm_q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_k): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_v): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (to_q): Linear(in_features=256, out_features=256, bias=False)
              (to_k): Linear(in_features=256, out_features=256, bias=False)
              (to_v): Linear(in_features=256, out_features=256, bias=False)
              (proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (reason): SimpleReasoning(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear): Conv1d(64, 64, kernel_size=(1,), stride=(1,), bias=False)
            )
            (enc_ffn): MLP(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (fc1): Linear(in_features=256, out_features=256, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=256, out_features=256, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (decoder): LPLayerDecoder(
            (attn1): AnyAttention(
              (norm_q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_k): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_v): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (to_q): Linear(in_features=256, out_features=256, bias=False)
              (to_k): Linear(in_features=256, out_features=256, bias=False)
              (to_v): Linear(in_features=256, out_features=256, bias=False)
              (proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (attn2): AnyAttention(
              (norm_q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_k): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_v): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (to_q): Linear(in_features=256, out_features=256, bias=False)
              (to_k): Linear(in_features=256, out_features=256, bias=False)
              (to_v): Linear(in_features=256, out_features=256, bias=False)
              (proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (ffn1): MLP(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (fc1): Linear(in_features=256, out_features=768, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=768, out_features=256, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ffn2): MLP(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (fc1): Linear(in_features=256, out_features=768, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=768, out_features=256, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (whole_qpos): PositionalEncoding(
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (1): LPBlock(
          (encoder): LPLayerEncoder(
            (enc_attn): AnyAttention(
              (norm_q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_k): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_v): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (to_q): Linear(in_features=256, out_features=256, bias=False)
              (to_k): Linear(in_features=256, out_features=256, bias=False)
              (to_v): Linear(in_features=256, out_features=256, bias=False)
              (proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (reason): SimpleReasoning(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear): Conv1d(64, 64, kernel_size=(1,), stride=(1,), bias=False)
            )
            (enc_ffn): MLP(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (fc1): Linear(in_features=256, out_features=256, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=256, out_features=256, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (decoder): LPLayerDecoder(
            (attn1): AnyAttention(
              (norm_q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_k): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_v): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (to_q): Linear(in_features=256, out_features=256, bias=False)
              (to_k): Linear(in_features=256, out_features=256, bias=False)
              (to_v): Linear(in_features=256, out_features=256, bias=False)
              (proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (attn2): AnyAttention(
              (norm_q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_k): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_v): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (to_q): Linear(in_features=256, out_features=256, bias=False)
              (to_k): Linear(in_features=256, out_features=256, bias=False)
              (to_v): Linear(in_features=256, out_features=256, bias=False)
              (proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (ffn1): MLP(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (fc1): Linear(in_features=256, out_features=768, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=768, out_features=256, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ffn2): MLP(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (fc1): Linear(in_features=256, out_features=768, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=768, out_features=256, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (whole_qpos): PositionalEncoding(
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (2): LPBlock(
          (encoder): LPLayerEncoder(
            (enc_attn): AnyAttention(
              (norm_q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_k): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_v): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (to_q): Linear(in_features=256, out_features=256, bias=False)
              (to_k): Linear(in_features=256, out_features=256, bias=False)
              (to_v): Linear(in_features=256, out_features=256, bias=False)
              (proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (reason): SimpleReasoning(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear): Conv1d(64, 64, kernel_size=(1,), stride=(1,), bias=False)
            )
            (enc_ffn): MLP(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (fc1): Linear(in_features=256, out_features=256, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=256, out_features=256, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (decoder): LPLayerDecoder(
            (attn1): AnyAttention(
              (norm_q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_k): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_v): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (to_q): Linear(in_features=256, out_features=256, bias=False)
              (to_k): Linear(in_features=256, out_features=256, bias=False)
              (to_v): Linear(in_features=256, out_features=256, bias=False)
              (proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (attn2): AnyAttention(
              (norm_q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_k): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_v): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (to_q): Linear(in_features=256, out_features=256, bias=False)
              (to_k): Linear(in_features=256, out_features=256, bias=False)
              (to_v): Linear(in_features=256, out_features=256, bias=False)
              (proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (ffn1): MLP(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (fc1): Linear(in_features=256, out_features=768, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=768, out_features=256, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ffn2): MLP(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (fc1): Linear(in_features=256, out_features=768, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=768, out_features=256, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (whole_qpos): PositionalEncoding(
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (3): LPBlock(
          (encoder): LPLayerEncoder(
            (enc_attn): AnyAttention(
              (norm_q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_k): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_v): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (to_q): Linear(in_features=256, out_features=256, bias=False)
              (to_k): Linear(in_features=256, out_features=256, bias=False)
              (to_v): Linear(in_features=256, out_features=256, bias=False)
              (proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (reason): SimpleReasoning(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear): Conv1d(64, 64, kernel_size=(1,), stride=(1,), bias=False)
            )
            (enc_ffn): MLP(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (fc1): Linear(in_features=256, out_features=256, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=256, out_features=256, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (decoder): LPLayerDecoder(
            (attn1): AnyAttention(
              (norm_q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_k): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_v): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (to_q): Linear(in_features=256, out_features=256, bias=False)
              (to_k): Linear(in_features=256, out_features=256, bias=False)
              (to_v): Linear(in_features=256, out_features=256, bias=False)
              (proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (attn2): AnyAttention(
              (norm_q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_k): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_v): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (to_q): Linear(in_features=256, out_features=256, bias=False)
              (to_k): Linear(in_features=256, out_features=256, bias=False)
              (to_v): Linear(in_features=256, out_features=256, bias=False)
              (proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (ffn1): MLP(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (fc1): Linear(in_features=256, out_features=768, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=768, out_features=256, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ffn2): MLP(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (fc1): Linear(in_features=256, out_features=768, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=768, out_features=256, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (whole_qpos): PositionalEncoding(
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (layer_2): Stage(
      (blocks): ModuleList(
        (0): LPBlock(
          (encoder): LPLayerEncoder(
            (enc_attn): AnyAttention(
              (norm_q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_k): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_v): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (to_q): Linear(in_features=256, out_features=256, bias=False)
              (to_k): Linear(in_features=256, out_features=256, bias=False)
              (to_v): Linear(in_features=256, out_features=256, bias=False)
              (proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (reason): SimpleReasoning(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear): Conv1d(64, 64, kernel_size=(1,), stride=(1,), bias=False)
            )
            (enc_ffn): MLP(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (fc1): Linear(in_features=256, out_features=256, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=256, out_features=256, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (decoder): LPLayerDecoder(
            (attn1): AnyAttention(
              (norm_q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_k): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_v): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (to_q): Linear(in_features=256, out_features=256, bias=False)
              (to_k): Linear(in_features=256, out_features=256, bias=False)
              (to_v): Linear(in_features=256, out_features=256, bias=False)
              (proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (attn2): AnyAttention(
              (norm_q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_k): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_v): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (to_q): Linear(in_features=256, out_features=256, bias=False)
              (to_k): Linear(in_features=256, out_features=256, bias=False)
              (to_v): Linear(in_features=256, out_features=256, bias=False)
              (proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (ffn1): MLP(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (fc1): Linear(in_features=256, out_features=768, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=768, out_features=256, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ffn2): MLP(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (fc1): Linear(in_features=256, out_features=768, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=768, out_features=256, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (whole_qpos): PositionalEncoding(
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (1): LPBlock(
          (encoder): LPLayerEncoder(
            (enc_attn): AnyAttention(
              (norm_q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_k): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_v): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (to_q): Linear(in_features=256, out_features=256, bias=False)
              (to_k): Linear(in_features=256, out_features=256, bias=False)
              (to_v): Linear(in_features=256, out_features=256, bias=False)
              (proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (reason): SimpleReasoning(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear): Conv1d(64, 64, kernel_size=(1,), stride=(1,), bias=False)
            )
            (enc_ffn): MLP(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (fc1): Linear(in_features=256, out_features=256, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=256, out_features=256, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (decoder): LPLayerDecoder(
            (attn1): AnyAttention(
              (norm_q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_k): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_v): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (to_q): Linear(in_features=256, out_features=256, bias=False)
              (to_k): Linear(in_features=256, out_features=256, bias=False)
              (to_v): Linear(in_features=256, out_features=256, bias=False)
              (proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (attn2): AnyAttention(
              (norm_q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_k): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_v): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (to_q): Linear(in_features=256, out_features=256, bias=False)
              (to_k): Linear(in_features=256, out_features=256, bias=False)
              (to_v): Linear(in_features=256, out_features=256, bias=False)
              (proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (ffn1): MLP(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (fc1): Linear(in_features=256, out_features=768, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=768, out_features=256, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ffn2): MLP(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (fc1): Linear(in_features=256, out_features=768, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=768, out_features=256, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (whole_qpos): PositionalEncoding(
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (2): LPBlock(
          (encoder): LPLayerEncoder(
            (enc_attn): AnyAttention(
              (norm_q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_k): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_v): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (to_q): Linear(in_features=256, out_features=256, bias=False)
              (to_k): Linear(in_features=256, out_features=256, bias=False)
              (to_v): Linear(in_features=256, out_features=256, bias=False)
              (proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (reason): SimpleReasoning(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear): Conv1d(64, 64, kernel_size=(1,), stride=(1,), bias=False)
            )
            (enc_ffn): MLP(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (fc1): Linear(in_features=256, out_features=256, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=256, out_features=256, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (decoder): LPLayerDecoder(
            (attn1): AnyAttention(
              (norm_q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_k): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_v): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (to_q): Linear(in_features=256, out_features=256, bias=False)
              (to_k): Linear(in_features=256, out_features=256, bias=False)
              (to_v): Linear(in_features=256, out_features=256, bias=False)
              (proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (attn2): AnyAttention(
              (norm_q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_k): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_v): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (to_q): Linear(in_features=256, out_features=256, bias=False)
              (to_k): Linear(in_features=256, out_features=256, bias=False)
              (to_v): Linear(in_features=256, out_features=256, bias=False)
              (proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (ffn1): MLP(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (fc1): Linear(in_features=256, out_features=768, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=768, out_features=256, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ffn2): MLP(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (fc1): Linear(in_features=256, out_features=768, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=768, out_features=256, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (whole_qpos): PositionalEncoding(
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (3): LPBlock(
          (encoder): LPLayerEncoder(
            (enc_attn): AnyAttention(
              (norm_q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_k): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_v): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (to_q): Linear(in_features=256, out_features=256, bias=False)
              (to_k): Linear(in_features=256, out_features=256, bias=False)
              (to_v): Linear(in_features=256, out_features=256, bias=False)
              (proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (reason): SimpleReasoning(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear): Conv1d(64, 64, kernel_size=(1,), stride=(1,), bias=False)
            )
            (enc_ffn): MLP(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (fc1): Linear(in_features=256, out_features=256, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=256, out_features=256, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (decoder): LPLayerDecoder(
            (attn1): AnyAttention(
              (norm_q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_k): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_v): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (to_q): Linear(in_features=256, out_features=256, bias=False)
              (to_k): Linear(in_features=256, out_features=256, bias=False)
              (to_v): Linear(in_features=256, out_features=256, bias=False)
              (proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (attn2): AnyAttention(
              (norm_q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_k): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_v): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (to_q): Linear(in_features=256, out_features=256, bias=False)
              (to_k): Linear(in_features=256, out_features=256, bias=False)
              (to_v): Linear(in_features=256, out_features=256, bias=False)
              (proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (ffn1): MLP(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (fc1): Linear(in_features=256, out_features=768, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=768, out_features=256, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ffn2): MLP(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (fc1): Linear(in_features=256, out_features=768, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=768, out_features=256, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (whole_qpos): PositionalEncoding(
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (4): LPBlock(
          (encoder): LPLayerEncoder(
            (enc_attn): AnyAttention(
              (norm_q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_k): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_v): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (to_q): Linear(in_features=256, out_features=256, bias=False)
              (to_k): Linear(in_features=256, out_features=256, bias=False)
              (to_v): Linear(in_features=256, out_features=256, bias=False)
              (proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (reason): SimpleReasoning(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear): Conv1d(64, 64, kernel_size=(1,), stride=(1,), bias=False)
            )
            (enc_ffn): MLP(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (fc1): Linear(in_features=256, out_features=256, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=256, out_features=256, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (decoder): LPLayerDecoder(
            (attn1): AnyAttention(
              (norm_q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_k): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_v): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (to_q): Linear(in_features=256, out_features=256, bias=False)
              (to_k): Linear(in_features=256, out_features=256, bias=False)
              (to_v): Linear(in_features=256, out_features=256, bias=False)
              (proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (attn2): AnyAttention(
              (norm_q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_k): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_v): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (to_q): Linear(in_features=256, out_features=256, bias=False)
              (to_k): Linear(in_features=256, out_features=256, bias=False)
              (to_v): Linear(in_features=256, out_features=256, bias=False)
              (proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (ffn1): MLP(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (fc1): Linear(in_features=256, out_features=768, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=768, out_features=256, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ffn2): MLP(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (fc1): Linear(in_features=256, out_features=768, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=768, out_features=256, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (whole_qpos): PositionalEncoding(
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (5): LPBlock(
          (encoder): LPLayerEncoder(
            (enc_attn): AnyAttention(
              (norm_q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_k): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_v): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (to_q): Linear(in_features=256, out_features=256, bias=False)
              (to_k): Linear(in_features=256, out_features=256, bias=False)
              (to_v): Linear(in_features=256, out_features=256, bias=False)
              (proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (reason): SimpleReasoning(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear): Conv1d(64, 64, kernel_size=(1,), stride=(1,), bias=False)
            )
            (enc_ffn): MLP(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (fc1): Linear(in_features=256, out_features=256, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=256, out_features=256, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (decoder): LPLayerDecoder(
            (attn1): AnyAttention(
              (norm_q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_k): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_v): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (to_q): Linear(in_features=256, out_features=256, bias=False)
              (to_k): Linear(in_features=256, out_features=256, bias=False)
              (to_v): Linear(in_features=256, out_features=256, bias=False)
              (proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (attn2): AnyAttention(
              (norm_q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_k): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_v): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (to_q): Linear(in_features=256, out_features=256, bias=False)
              (to_k): Linear(in_features=256, out_features=256, bias=False)
              (to_v): Linear(in_features=256, out_features=256, bias=False)
              (proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (ffn1): MLP(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (fc1): Linear(in_features=256, out_features=768, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=768, out_features=256, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ffn2): MLP(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (fc1): Linear(in_features=256, out_features=768, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=768, out_features=256, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (whole_qpos): PositionalEncoding(
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (layer_3): Stage(
      (blocks): ModuleList(
        (0): LPBlock(
          (encoder): LPLayerEncoder(
            (enc_attn): AnyAttention(
              (norm_q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_k): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_v): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (to_q): Linear(in_features=256, out_features=256, bias=False)
              (to_k): Linear(in_features=256, out_features=256, bias=False)
              (to_v): Linear(in_features=256, out_features=256, bias=False)
              (proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (reason): SimpleReasoning(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear): Conv1d(64, 64, kernel_size=(1,), stride=(1,), bias=False)
            )
            (enc_ffn): MLP(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (fc1): Linear(in_features=256, out_features=256, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=256, out_features=256, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (decoder): LPLayerDecoder(
            (attn1): AnyAttention(
              (norm_q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_k): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_v): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (to_q): Linear(in_features=256, out_features=256, bias=False)
              (to_k): Linear(in_features=256, out_features=256, bias=False)
              (to_v): Linear(in_features=256, out_features=256, bias=False)
              (proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (attn2): AnyAttention(
              (norm_q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_k): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_v): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (to_q): Linear(in_features=256, out_features=256, bias=False)
              (to_k): Linear(in_features=256, out_features=256, bias=False)
              (to_v): Linear(in_features=256, out_features=256, bias=False)
              (proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (ffn1): MLP(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (fc1): Linear(in_features=256, out_features=768, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=768, out_features=256, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ffn2): MLP(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (fc1): Linear(in_features=256, out_features=768, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=768, out_features=256, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (whole_qpos): PositionalEncoding(
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (1): LPBlock(
          (encoder): LPLayerEncoder(
            (enc_attn): AnyAttention(
              (norm_q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_k): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_v): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (to_q): Linear(in_features=256, out_features=256, bias=False)
              (to_k): Linear(in_features=256, out_features=256, bias=False)
              (to_v): Linear(in_features=256, out_features=256, bias=False)
              (proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (reason): SimpleReasoning(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear): Conv1d(64, 64, kernel_size=(1,), stride=(1,), bias=False)
            )
            (enc_ffn): MLP(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (fc1): Linear(in_features=256, out_features=256, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=256, out_features=256, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (decoder): LPLayerDecoder(
            (attn1): AnyAttention(
              (norm_q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_k): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_v): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (to_q): Linear(in_features=256, out_features=256, bias=False)
              (to_k): Linear(in_features=256, out_features=256, bias=False)
              (to_v): Linear(in_features=256, out_features=256, bias=False)
              (proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (attn2): AnyAttention(
              (norm_q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_k): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_v): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (to_q): Linear(in_features=256, out_features=256, bias=False)
              (to_k): Linear(in_features=256, out_features=256, bias=False)
              (to_v): Linear(in_features=256, out_features=256, bias=False)
              (proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (ffn1): MLP(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (fc1): Linear(in_features=256, out_features=768, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=768, out_features=256, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ffn2): MLP(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (fc1): Linear(in_features=256, out_features=768, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=768, out_features=256, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (whole_qpos): PositionalEncoding(
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (2): LPBlock(
          (encoder): LPLayerEncoder(
            (enc_attn): AnyAttention(
              (norm_q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_k): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_v): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (to_q): Linear(in_features=256, out_features=256, bias=False)
              (to_k): Linear(in_features=256, out_features=256, bias=False)
              (to_v): Linear(in_features=256, out_features=256, bias=False)
              (proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (reason): SimpleReasoning(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear): Conv1d(64, 64, kernel_size=(1,), stride=(1,), bias=False)
            )
            (enc_ffn): MLP(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (fc1): Linear(in_features=256, out_features=256, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=256, out_features=256, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (decoder): LPLayerDecoder(
            (attn1): AnyAttention(
              (norm_q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_k): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_v): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (to_q): Linear(in_features=256, out_features=256, bias=False)
              (to_k): Linear(in_features=256, out_features=256, bias=False)
              (to_v): Linear(in_features=256, out_features=256, bias=False)
              (proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (attn2): AnyAttention(
              (norm_q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_k): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_v): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (to_q): Linear(in_features=256, out_features=256, bias=False)
              (to_k): Linear(in_features=256, out_features=256, bias=False)
              (to_v): Linear(in_features=256, out_features=256, bias=False)
              (proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (ffn1): MLP(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (fc1): Linear(in_features=256, out_features=768, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=768, out_features=256, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ffn2): MLP(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (fc1): Linear(in_features=256, out_features=768, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=768, out_features=256, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (whole_qpos): PositionalEncoding(
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (3): LPBlock(
          (encoder): LPLayerEncoder(
            (enc_attn): AnyAttention(
              (norm_q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_k): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_v): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (to_q): Linear(in_features=256, out_features=256, bias=False)
              (to_k): Linear(in_features=256, out_features=256, bias=False)
              (to_v): Linear(in_features=256, out_features=256, bias=False)
              (proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (reason): SimpleReasoning(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear): Conv1d(64, 64, kernel_size=(1,), stride=(1,), bias=False)
            )
            (enc_ffn): MLP(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (fc1): Linear(in_features=256, out_features=256, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=256, out_features=256, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (decoder): LPLayerDecoder(
            (attn1): AnyAttention(
              (norm_q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_k): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_v): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (to_q): Linear(in_features=256, out_features=256, bias=False)
              (to_k): Linear(in_features=256, out_features=256, bias=False)
              (to_v): Linear(in_features=256, out_features=256, bias=False)
              (proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (attn2): AnyAttention(
              (norm_q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_k): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_v): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (to_q): Linear(in_features=256, out_features=256, bias=False)
              (to_k): Linear(in_features=256, out_features=256, bias=False)
              (to_v): Linear(in_features=256, out_features=256, bias=False)
              (proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (ffn1): MLP(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (fc1): Linear(in_features=256, out_features=768, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=768, out_features=256, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ffn2): MLP(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (fc1): Linear(in_features=256, out_features=768, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=768, out_features=256, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (whole_qpos): PositionalEncoding(
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (4): LPBlock(
          (encoder): LPLayerEncoder(
            (enc_attn): AnyAttention(
              (norm_q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_k): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_v): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (to_q): Linear(in_features=256, out_features=256, bias=False)
              (to_k): Linear(in_features=256, out_features=256, bias=False)
              (to_v): Linear(in_features=256, out_features=256, bias=False)
              (proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (reason): SimpleReasoning(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear): Conv1d(64, 64, kernel_size=(1,), stride=(1,), bias=False)
            )
            (enc_ffn): MLP(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (fc1): Linear(in_features=256, out_features=256, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=256, out_features=256, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (decoder): LPLayerDecoder(
            (attn1): AnyAttention(
              (norm_q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_k): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_v): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (to_q): Linear(in_features=256, out_features=256, bias=False)
              (to_k): Linear(in_features=256, out_features=256, bias=False)
              (to_v): Linear(in_features=256, out_features=256, bias=False)
              (proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (attn2): AnyAttention(
              (norm_q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_k): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_v): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (to_q): Linear(in_features=256, out_features=256, bias=False)
              (to_k): Linear(in_features=256, out_features=256, bias=False)
              (to_v): Linear(in_features=256, out_features=256, bias=False)
              (proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (ffn1): MLP(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (fc1): Linear(in_features=256, out_features=768, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=768, out_features=256, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ffn2): MLP(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (fc1): Linear(in_features=256, out_features=768, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=768, out_features=256, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (whole_qpos): PositionalEncoding(
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (5): LPBlock(
          (encoder): LPLayerEncoder(
            (enc_attn): AnyAttention(
              (norm_q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_k): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_v): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (to_q): Linear(in_features=256, out_features=256, bias=False)
              (to_k): Linear(in_features=256, out_features=256, bias=False)
              (to_v): Linear(in_features=256, out_features=256, bias=False)
              (proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (reason): SimpleReasoning(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear): Conv1d(64, 64, kernel_size=(1,), stride=(1,), bias=False)
            )
            (enc_ffn): MLP(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (fc1): Linear(in_features=256, out_features=256, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=256, out_features=256, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (decoder): LPLayerDecoder(
            (attn1): AnyAttention(
              (norm_q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_k): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_v): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (to_q): Linear(in_features=256, out_features=256, bias=False)
              (to_k): Linear(in_features=256, out_features=256, bias=False)
              (to_v): Linear(in_features=256, out_features=256, bias=False)
              (proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (attn2): AnyAttention(
              (norm_q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_k): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_v): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (to_q): Linear(in_features=256, out_features=256, bias=False)
              (to_k): Linear(in_features=256, out_features=256, bias=False)
              (to_v): Linear(in_features=256, out_features=256, bias=False)
              (proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (ffn1): MLP(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (fc1): Linear(in_features=256, out_features=768, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=768, out_features=256, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ffn2): MLP(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (fc1): Linear(in_features=256, out_features=768, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=768, out_features=256, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (whole_qpos): PositionalEncoding(
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (last_encoder): LPLayerEncoder(
        (enc_attn): AnyAttention(
          (norm_q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm_k): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm_v): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (to_q): Linear(in_features=256, out_features=256, bias=False)
          (to_k): Linear(in_features=256, out_features=256, bias=False)
          (to_v): Linear(in_features=256, out_features=256, bias=False)
          (proj): Linear(in_features=256, out_features=256, bias=True)
        )
        (reason): SimpleReasoning(
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (linear): Conv1d(64, 64, kernel_size=(1,), stride=(1,), bias=False)
        )
      )
    )
  )
  (decoder): TransformerDecoder(
    (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
        )
        (linear1): Linear(in_features=256, out_features=1024, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=1024, out_features=256, bias=True)
        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
        )
        (linear1): Linear(in_features=256, out_features=1024, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=1024, out_features=256, bias=True)
        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
        )
        (linear1): Linear(in_features=256, out_features=1024, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=1024, out_features=256, bias=True)
        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
        )
        (linear1): Linear(in_features=256, out_features=1024, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=1024, out_features=256, bias=True)
        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
        )
        (linear1): Linear(in_features=256, out_features=1024, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=1024, out_features=256, bias=True)
        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
        )
        (linear1): Linear(in_features=256, out_features=1024, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=1024, out_features=256, bias=True)
        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (linear): Linear(in_features=256, out_features=10, bias=True)
  (softmax): LogSoftmax(dim=-1)
)
Namespace(adv_lambda=0.01, adv_lr=0.01, adv_theta=0.01, batch_size=256, cat_xm=False, checkpoint_every=4, checkpoint_path='../weights/language_parser/scan/defaults_mcd1.pt', d_model=256, dataset='scan', dim_feedforward=1024, dropout=0.1, filter=256, learning_rate=4e-05, load_weights_from=None, model_type='language_parser', n_layers=6, nhead=8, num_epochs=150, num_runs=1, out_attn_wts='train_defaults_mcd1_attn_maps', out_data_file='train_defaults_mcd1', pos=False, record_loss_every=20, results_dir='language_parser', split='mcd1', use_adversary=False, use_xv=False)
Model size: 40992266
LanguageParser(
  (src_embedding): Embedding(17, 256)
  (trg_embedding): Embedding(10, 256)
  (positional_encoding): PositionalEncoding(
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (encoder): LPEncoder(
    (act): GELU()
    (layer_0): Stage(
      (blocks): ModuleList(
        (0): LPBlock(
          (encoder): LPLayerEncoder(
            (enc_attn): AnyAttention(
              (norm_q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_k): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_v): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (to_q): Linear(in_features=256, out_features=256, bias=False)
              (to_k): Linear(in_features=256, out_features=256, bias=False)
              (to_v): Linear(in_features=256, out_features=256, bias=False)
              (proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (reason): SimpleReasoning(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear): Conv1d(64, 64, kernel_size=(1,), stride=(1,), bias=False)
            )
            (enc_ffn): MLP(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (fc1): Linear(in_features=256, out_features=256, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=256, out_features=256, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (decoder): LPLayerDecoder(
            (attn1): AnyAttention(
              (norm_q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_k): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_v): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (to_q): Linear(in_features=256, out_features=256, bias=False)
              (to_k): Linear(in_features=256, out_features=256, bias=False)
              (to_v): Linear(in_features=256, out_features=256, bias=False)
              (proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (attn2): AnyAttention(
              (norm_q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_k): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_v): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (to_q): Linear(in_features=256, out_features=256, bias=False)
              (to_k): Linear(in_features=256, out_features=256, bias=False)
              (to_v): Linear(in_features=256, out_features=256, bias=False)
              (proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (ffn1): MLP(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (fc1): Linear(in_features=256, out_features=768, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=768, out_features=256, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ffn2): MLP(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (fc1): Linear(in_features=256, out_features=768, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=768, out_features=256, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (whole_qpos): PositionalEncoding(
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (1): LPBlock(
          (encoder): LPLayerEncoder(
            (enc_attn): AnyAttention(
              (norm_q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_k): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_v): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (to_q): Linear(in_features=256, out_features=256, bias=False)
              (to_k): Linear(in_features=256, out_features=256, bias=False)
              (to_v): Linear(in_features=256, out_features=256, bias=False)
              (proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (reason): SimpleReasoning(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear): Conv1d(64, 64, kernel_size=(1,), stride=(1,), bias=False)
            )
            (enc_ffn): MLP(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (fc1): Linear(in_features=256, out_features=256, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=256, out_features=256, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (decoder): LPLayerDecoder(
            (attn1): AnyAttention(
              (norm_q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_k): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_v): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (to_q): Linear(in_features=256, out_features=256, bias=False)
              (to_k): Linear(in_features=256, out_features=256, bias=False)
              (to_v): Linear(in_features=256, out_features=256, bias=False)
              (proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (attn2): AnyAttention(
              (norm_q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_k): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_v): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (to_q): Linear(in_features=256, out_features=256, bias=False)
              (to_k): Linear(in_features=256, out_features=256, bias=False)
              (to_v): Linear(in_features=256, out_features=256, bias=False)
              (proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (ffn1): MLP(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (fc1): Linear(in_features=256, out_features=768, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=768, out_features=256, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ffn2): MLP(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (fc1): Linear(in_features=256, out_features=768, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=768, out_features=256, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (whole_qpos): PositionalEncoding(
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (2): LPBlock(
          (encoder): LPLayerEncoder(
            (enc_attn): AnyAttention(
              (norm_q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_k): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_v): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (to_q): Linear(in_features=256, out_features=256, bias=False)
              (to_k): Linear(in_features=256, out_features=256, bias=False)
              (to_v): Linear(in_features=256, out_features=256, bias=False)
              (proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (reason): SimpleReasoning(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear): Conv1d(64, 64, kernel_size=(1,), stride=(1,), bias=False)
            )
            (enc_ffn): MLP(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (fc1): Linear(in_features=256, out_features=256, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=256, out_features=256, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (decoder): LPLayerDecoder(
            (attn1): AnyAttention(
              (norm_q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_k): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_v): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (to_q): Linear(in_features=256, out_features=256, bias=False)
              (to_k): Linear(in_features=256, out_features=256, bias=False)
              (to_v): Linear(in_features=256, out_features=256, bias=False)
              (proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (attn2): AnyAttention(
              (norm_q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_k): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_v): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (to_q): Linear(in_features=256, out_features=256, bias=False)
              (to_k): Linear(in_features=256, out_features=256, bias=False)
              (to_v): Linear(in_features=256, out_features=256, bias=False)
              (proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (ffn1): MLP(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (fc1): Linear(in_features=256, out_features=768, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=768, out_features=256, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ffn2): MLP(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (fc1): Linear(in_features=256, out_features=768, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=768, out_features=256, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (whole_qpos): PositionalEncoding(
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (3): LPBlock(
          (encoder): LPLayerEncoder(
            (enc_attn): AnyAttention(
              (norm_q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_k): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_v): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (to_q): Linear(in_features=256, out_features=256, bias=False)
              (to_k): Linear(in_features=256, out_features=256, bias=False)
              (to_v): Linear(in_features=256, out_features=256, bias=False)
              (proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (reason): SimpleReasoning(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear): Conv1d(64, 64, kernel_size=(1,), stride=(1,), bias=False)
            )
            (enc_ffn): MLP(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (fc1): Linear(in_features=256, out_features=256, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=256, out_features=256, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (decoder): LPLayerDecoder(
            (attn1): AnyAttention(
              (norm_q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_k): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_v): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (to_q): Linear(in_features=256, out_features=256, bias=False)
              (to_k): Linear(in_features=256, out_features=256, bias=False)
              (to_v): Linear(in_features=256, out_features=256, bias=False)
              (proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (attn2): AnyAttention(
              (norm_q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_k): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_v): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (to_q): Linear(in_features=256, out_features=256, bias=False)
              (to_k): Linear(in_features=256, out_features=256, bias=False)
              (to_v): Linear(in_features=256, out_features=256, bias=False)
              (proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (ffn1): MLP(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (fc1): Linear(in_features=256, out_features=768, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=768, out_features=256, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ffn2): MLP(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (fc1): Linear(in_features=256, out_features=768, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=768, out_features=256, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (whole_qpos): PositionalEncoding(
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (layer_1): Stage(
      (blocks): ModuleList(
        (0): LPBlock(
          (encoder): LPLayerEncoder(
            (enc_attn): AnyAttention(
              (norm_q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_k): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_v): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (to_q): Linear(in_features=256, out_features=256, bias=False)
              (to_k): Linear(in_features=256, out_features=256, bias=False)
              (to_v): Linear(in_features=256, out_features=256, bias=False)
              (proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (reason): SimpleReasoning(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear): Conv1d(64, 64, kernel_size=(1,), stride=(1,), bias=False)
            )
            (enc_ffn): MLP(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (fc1): Linear(in_features=256, out_features=256, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=256, out_features=256, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (decoder): LPLayerDecoder(
            (attn1): AnyAttention(
              (norm_q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_k): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_v): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (to_q): Linear(in_features=256, out_features=256, bias=False)
              (to_k): Linear(in_features=256, out_features=256, bias=False)
              (to_v): Linear(in_features=256, out_features=256, bias=False)
              (proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (attn2): AnyAttention(
              (norm_q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_k): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_v): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (to_q): Linear(in_features=256, out_features=256, bias=False)
              (to_k): Linear(in_features=256, out_features=256, bias=False)
              (to_v): Linear(in_features=256, out_features=256, bias=False)
              (proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (ffn1): MLP(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (fc1): Linear(in_features=256, out_features=768, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=768, out_features=256, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ffn2): MLP(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (fc1): Linear(in_features=256, out_features=768, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=768, out_features=256, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (whole_qpos): PositionalEncoding(
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (1): LPBlock(
          (encoder): LPLayerEncoder(
            (enc_attn): AnyAttention(
              (norm_q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_k): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_v): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (to_q): Linear(in_features=256, out_features=256, bias=False)
              (to_k): Linear(in_features=256, out_features=256, bias=False)
              (to_v): Linear(in_features=256, out_features=256, bias=False)
              (proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (reason): SimpleReasoning(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear): Conv1d(64, 64, kernel_size=(1,), stride=(1,), bias=False)
            )
            (enc_ffn): MLP(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (fc1): Linear(in_features=256, out_features=256, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=256, out_features=256, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (decoder): LPLayerDecoder(
            (attn1): AnyAttention(
              (norm_q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_k): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_v): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (to_q): Linear(in_features=256, out_features=256, bias=False)
              (to_k): Linear(in_features=256, out_features=256, bias=False)
              (to_v): Linear(in_features=256, out_features=256, bias=False)
              (proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (attn2): AnyAttention(
              (norm_q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_k): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_v): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (to_q): Linear(in_features=256, out_features=256, bias=False)
              (to_k): Linear(in_features=256, out_features=256, bias=False)
              (to_v): Linear(in_features=256, out_features=256, bias=False)
              (proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (ffn1): MLP(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (fc1): Linear(in_features=256, out_features=768, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=768, out_features=256, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ffn2): MLP(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (fc1): Linear(in_features=256, out_features=768, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=768, out_features=256, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (whole_qpos): PositionalEncoding(
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (2): LPBlock(
          (encoder): LPLayerEncoder(
            (enc_attn): AnyAttention(
              (norm_q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_k): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_v): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (to_q): Linear(in_features=256, out_features=256, bias=False)
              (to_k): Linear(in_features=256, out_features=256, bias=False)
              (to_v): Linear(in_features=256, out_features=256, bias=False)
              (proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (reason): SimpleReasoning(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear): Conv1d(64, 64, kernel_size=(1,), stride=(1,), bias=False)
            )
            (enc_ffn): MLP(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (fc1): Linear(in_features=256, out_features=256, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=256, out_features=256, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (decoder): LPLayerDecoder(
            (attn1): AnyAttention(
              (norm_q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_k): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_v): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (to_q): Linear(in_features=256, out_features=256, bias=False)
              (to_k): Linear(in_features=256, out_features=256, bias=False)
              (to_v): Linear(in_features=256, out_features=256, bias=False)
              (proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (attn2): AnyAttention(
              (norm_q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_k): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_v): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (to_q): Linear(in_features=256, out_features=256, bias=False)
              (to_k): Linear(in_features=256, out_features=256, bias=False)
              (to_v): Linear(in_features=256, out_features=256, bias=False)
              (proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (ffn1): MLP(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (fc1): Linear(in_features=256, out_features=768, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=768, out_features=256, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ffn2): MLP(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (fc1): Linear(in_features=256, out_features=768, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=768, out_features=256, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (whole_qpos): PositionalEncoding(
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (3): LPBlock(
          (encoder): LPLayerEncoder(
            (enc_attn): AnyAttention(
              (norm_q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_k): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_v): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (to_q): Linear(in_features=256, out_features=256, bias=False)
              (to_k): Linear(in_features=256, out_features=256, bias=False)
              (to_v): Linear(in_features=256, out_features=256, bias=False)
              (proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (reason): SimpleReasoning(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear): Conv1d(64, 64, kernel_size=(1,), stride=(1,), bias=False)
            )
            (enc_ffn): MLP(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (fc1): Linear(in_features=256, out_features=256, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=256, out_features=256, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (decoder): LPLayerDecoder(
            (attn1): AnyAttention(
              (norm_q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_k): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_v): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (to_q): Linear(in_features=256, out_features=256, bias=False)
              (to_k): Linear(in_features=256, out_features=256, bias=False)
              (to_v): Linear(in_features=256, out_features=256, bias=False)
              (proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (attn2): AnyAttention(
              (norm_q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_k): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_v): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (to_q): Linear(in_features=256, out_features=256, bias=False)
              (to_k): Linear(in_features=256, out_features=256, bias=False)
              (to_v): Linear(in_features=256, out_features=256, bias=False)
              (proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (ffn1): MLP(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (fc1): Linear(in_features=256, out_features=768, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=768, out_features=256, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ffn2): MLP(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (fc1): Linear(in_features=256, out_features=768, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=768, out_features=256, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (whole_qpos): PositionalEncoding(
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (layer_2): Stage(
      (blocks): ModuleList(
        (0): LPBlock(
          (encoder): LPLayerEncoder(
            (enc_attn): AnyAttention(
              (norm_q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_k): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_v): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (to_q): Linear(in_features=256, out_features=256, bias=False)
              (to_k): Linear(in_features=256, out_features=256, bias=False)
              (to_v): Linear(in_features=256, out_features=256, bias=False)
              (proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (reason): SimpleReasoning(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear): Conv1d(64, 64, kernel_size=(1,), stride=(1,), bias=False)
            )
            (enc_ffn): MLP(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (fc1): Linear(in_features=256, out_features=256, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=256, out_features=256, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (decoder): LPLayerDecoder(
            (attn1): AnyAttention(
              (norm_q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_k): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_v): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (to_q): Linear(in_features=256, out_features=256, bias=False)
              (to_k): Linear(in_features=256, out_features=256, bias=False)
              (to_v): Linear(in_features=256, out_features=256, bias=False)
              (proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (attn2): AnyAttention(
              (norm_q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_k): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_v): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (to_q): Linear(in_features=256, out_features=256, bias=False)
              (to_k): Linear(in_features=256, out_features=256, bias=False)
              (to_v): Linear(in_features=256, out_features=256, bias=False)
              (proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (ffn1): MLP(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (fc1): Linear(in_features=256, out_features=768, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=768, out_features=256, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ffn2): MLP(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (fc1): Linear(in_features=256, out_features=768, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=768, out_features=256, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (whole_qpos): PositionalEncoding(
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (1): LPBlock(
          (encoder): LPLayerEncoder(
            (enc_attn): AnyAttention(
              (norm_q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_k): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_v): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (to_q): Linear(in_features=256, out_features=256, bias=False)
              (to_k): Linear(in_features=256, out_features=256, bias=False)
              (to_v): Linear(in_features=256, out_features=256, bias=False)
              (proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (reason): SimpleReasoning(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear): Conv1d(64, 64, kernel_size=(1,), stride=(1,), bias=False)
            )
            (enc_ffn): MLP(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (fc1): Linear(in_features=256, out_features=256, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=256, out_features=256, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (decoder): LPLayerDecoder(
            (attn1): AnyAttention(
              (norm_q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_k): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_v): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (to_q): Linear(in_features=256, out_features=256, bias=False)
              (to_k): Linear(in_features=256, out_features=256, bias=False)
              (to_v): Linear(in_features=256, out_features=256, bias=False)
              (proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (attn2): AnyAttention(
              (norm_q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_k): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_v): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (to_q): Linear(in_features=256, out_features=256, bias=False)
              (to_k): Linear(in_features=256, out_features=256, bias=False)
              (to_v): Linear(in_features=256, out_features=256, bias=False)
              (proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (ffn1): MLP(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (fc1): Linear(in_features=256, out_features=768, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=768, out_features=256, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ffn2): MLP(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (fc1): Linear(in_features=256, out_features=768, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=768, out_features=256, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (whole_qpos): PositionalEncoding(
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (2): LPBlock(
          (encoder): LPLayerEncoder(
            (enc_attn): AnyAttention(
              (norm_q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_k): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_v): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (to_q): Linear(in_features=256, out_features=256, bias=False)
              (to_k): Linear(in_features=256, out_features=256, bias=False)
              (to_v): Linear(in_features=256, out_features=256, bias=False)
              (proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (reason): SimpleReasoning(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear): Conv1d(64, 64, kernel_size=(1,), stride=(1,), bias=False)
            )
            (enc_ffn): MLP(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (fc1): Linear(in_features=256, out_features=256, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=256, out_features=256, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (decoder): LPLayerDecoder(
            (attn1): AnyAttention(
              (norm_q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_k): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_v): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (to_q): Linear(in_features=256, out_features=256, bias=False)
              (to_k): Linear(in_features=256, out_features=256, bias=False)
              (to_v): Linear(in_features=256, out_features=256, bias=False)
              (proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (attn2): AnyAttention(
              (norm_q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_k): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_v): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (to_q): Linear(in_features=256, out_features=256, bias=False)
              (to_k): Linear(in_features=256, out_features=256, bias=False)
              (to_v): Linear(in_features=256, out_features=256, bias=False)
              (proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (ffn1): MLP(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (fc1): Linear(in_features=256, out_features=768, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=768, out_features=256, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ffn2): MLP(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (fc1): Linear(in_features=256, out_features=768, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=768, out_features=256, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (whole_qpos): PositionalEncoding(
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (3): LPBlock(
          (encoder): LPLayerEncoder(
            (enc_attn): AnyAttention(
              (norm_q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_k): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_v): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (to_q): Linear(in_features=256, out_features=256, bias=False)
              (to_k): Linear(in_features=256, out_features=256, bias=False)
              (to_v): Linear(in_features=256, out_features=256, bias=False)
              (proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (reason): SimpleReasoning(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear): Conv1d(64, 64, kernel_size=(1,), stride=(1,), bias=False)
            )
            (enc_ffn): MLP(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (fc1): Linear(in_features=256, out_features=256, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=256, out_features=256, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (decoder): LPLayerDecoder(
            (attn1): AnyAttention(
              (norm_q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_k): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_v): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (to_q): Linear(in_features=256, out_features=256, bias=False)
              (to_k): Linear(in_features=256, out_features=256, bias=False)
              (to_v): Linear(in_features=256, out_features=256, bias=False)
              (proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (attn2): AnyAttention(
              (norm_q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_k): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_v): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (to_q): Linear(in_features=256, out_features=256, bias=False)
              (to_k): Linear(in_features=256, out_features=256, bias=False)
              (to_v): Linear(in_features=256, out_features=256, bias=False)
              (proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (ffn1): MLP(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (fc1): Linear(in_features=256, out_features=768, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=768, out_features=256, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ffn2): MLP(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (fc1): Linear(in_features=256, out_features=768, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=768, out_features=256, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (whole_qpos): PositionalEncoding(
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (4): LPBlock(
          (encoder): LPLayerEncoder(
            (enc_attn): AnyAttention(
              (norm_q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_k): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_v): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (to_q): Linear(in_features=256, out_features=256, bias=False)
              (to_k): Linear(in_features=256, out_features=256, bias=False)
              (to_v): Linear(in_features=256, out_features=256, bias=False)
              (proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (reason): SimpleReasoning(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear): Conv1d(64, 64, kernel_size=(1,), stride=(1,), bias=False)
            )
            (enc_ffn): MLP(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (fc1): Linear(in_features=256, out_features=256, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=256, out_features=256, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (decoder): LPLayerDecoder(
            (attn1): AnyAttention(
              (norm_q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_k): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_v): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (to_q): Linear(in_features=256, out_features=256, bias=False)
              (to_k): Linear(in_features=256, out_features=256, bias=False)
              (to_v): Linear(in_features=256, out_features=256, bias=False)
              (proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (attn2): AnyAttention(
              (norm_q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_k): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_v): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (to_q): Linear(in_features=256, out_features=256, bias=False)
              (to_k): Linear(in_features=256, out_features=256, bias=False)
              (to_v): Linear(in_features=256, out_features=256, bias=False)
              (proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (ffn1): MLP(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (fc1): Linear(in_features=256, out_features=768, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=768, out_features=256, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ffn2): MLP(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (fc1): Linear(in_features=256, out_features=768, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=768, out_features=256, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (whole_qpos): PositionalEncoding(
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (5): LPBlock(
          (encoder): LPLayerEncoder(
            (enc_attn): AnyAttention(
              (norm_q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_k): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_v): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (to_q): Linear(in_features=256, out_features=256, bias=False)
              (to_k): Linear(in_features=256, out_features=256, bias=False)
              (to_v): Linear(in_features=256, out_features=256, bias=False)
              (proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (reason): SimpleReasoning(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear): Conv1d(64, 64, kernel_size=(1,), stride=(1,), bias=False)
            )
            (enc_ffn): MLP(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (fc1): Linear(in_features=256, out_features=256, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=256, out_features=256, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (decoder): LPLayerDecoder(
            (attn1): AnyAttention(
              (norm_q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_k): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_v): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (to_q): Linear(in_features=256, out_features=256, bias=False)
              (to_k): Linear(in_features=256, out_features=256, bias=False)
              (to_v): Linear(in_features=256, out_features=256, bias=False)
              (proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (attn2): AnyAttention(
              (norm_q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_k): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_v): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (to_q): Linear(in_features=256, out_features=256, bias=False)
              (to_k): Linear(in_features=256, out_features=256, bias=False)
              (to_v): Linear(in_features=256, out_features=256, bias=False)
              (proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (ffn1): MLP(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (fc1): Linear(in_features=256, out_features=768, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=768, out_features=256, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ffn2): MLP(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (fc1): Linear(in_features=256, out_features=768, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=768, out_features=256, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (whole_qpos): PositionalEncoding(
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (layer_3): Stage(
      (blocks): ModuleList(
        (0): LPBlock(
          (encoder): LPLayerEncoder(
            (enc_attn): AnyAttention(
              (norm_q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_k): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_v): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (to_q): Linear(in_features=256, out_features=256, bias=False)
              (to_k): Linear(in_features=256, out_features=256, bias=False)
              (to_v): Linear(in_features=256, out_features=256, bias=False)
              (proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (reason): SimpleReasoning(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear): Conv1d(64, 64, kernel_size=(1,), stride=(1,), bias=False)
            )
            (enc_ffn): MLP(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (fc1): Linear(in_features=256, out_features=256, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=256, out_features=256, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (decoder): LPLayerDecoder(
            (attn1): AnyAttention(
              (norm_q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_k): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_v): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (to_q): Linear(in_features=256, out_features=256, bias=False)
              (to_k): Linear(in_features=256, out_features=256, bias=False)
              (to_v): Linear(in_features=256, out_features=256, bias=False)
              (proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (attn2): AnyAttention(
              (norm_q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_k): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_v): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (to_q): Linear(in_features=256, out_features=256, bias=False)
              (to_k): Linear(in_features=256, out_features=256, bias=False)
              (to_v): Linear(in_features=256, out_features=256, bias=False)
              (proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (ffn1): MLP(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (fc1): Linear(in_features=256, out_features=768, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=768, out_features=256, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ffn2): MLP(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (fc1): Linear(in_features=256, out_features=768, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=768, out_features=256, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (whole_qpos): PositionalEncoding(
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (1): LPBlock(
          (encoder): LPLayerEncoder(
            (enc_attn): AnyAttention(
              (norm_q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_k): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_v): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (to_q): Linear(in_features=256, out_features=256, bias=False)
              (to_k): Linear(in_features=256, out_features=256, bias=False)
              (to_v): Linear(in_features=256, out_features=256, bias=False)
              (proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (reason): SimpleReasoning(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear): Conv1d(64, 64, kernel_size=(1,), stride=(1,), bias=False)
            )
            (enc_ffn): MLP(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (fc1): Linear(in_features=256, out_features=256, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=256, out_features=256, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (decoder): LPLayerDecoder(
            (attn1): AnyAttention(
              (norm_q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_k): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_v): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (to_q): Linear(in_features=256, out_features=256, bias=False)
              (to_k): Linear(in_features=256, out_features=256, bias=False)
              (to_v): Linear(in_features=256, out_features=256, bias=False)
              (proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (attn2): AnyAttention(
              (norm_q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_k): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_v): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (to_q): Linear(in_features=256, out_features=256, bias=False)
              (to_k): Linear(in_features=256, out_features=256, bias=False)
              (to_v): Linear(in_features=256, out_features=256, bias=False)
              (proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (ffn1): MLP(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (fc1): Linear(in_features=256, out_features=768, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=768, out_features=256, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ffn2): MLP(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (fc1): Linear(in_features=256, out_features=768, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=768, out_features=256, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (whole_qpos): PositionalEncoding(
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (2): LPBlock(
          (encoder): LPLayerEncoder(
            (enc_attn): AnyAttention(
              (norm_q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_k): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_v): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (to_q): Linear(in_features=256, out_features=256, bias=False)
              (to_k): Linear(in_features=256, out_features=256, bias=False)
              (to_v): Linear(in_features=256, out_features=256, bias=False)
              (proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (reason): SimpleReasoning(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear): Conv1d(64, 64, kernel_size=(1,), stride=(1,), bias=False)
            )
            (enc_ffn): MLP(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (fc1): Linear(in_features=256, out_features=256, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=256, out_features=256, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (decoder): LPLayerDecoder(
            (attn1): AnyAttention(
              (norm_q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_k): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_v): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (to_q): Linear(in_features=256, out_features=256, bias=False)
              (to_k): Linear(in_features=256, out_features=256, bias=False)
              (to_v): Linear(in_features=256, out_features=256, bias=False)
              (proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (attn2): AnyAttention(
              (norm_q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_k): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_v): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (to_q): Linear(in_features=256, out_features=256, bias=False)
              (to_k): Linear(in_features=256, out_features=256, bias=False)
              (to_v): Linear(in_features=256, out_features=256, bias=False)
              (proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (ffn1): MLP(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (fc1): Linear(in_features=256, out_features=768, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=768, out_features=256, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ffn2): MLP(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (fc1): Linear(in_features=256, out_features=768, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=768, out_features=256, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (whole_qpos): PositionalEncoding(
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (3): LPBlock(
          (encoder): LPLayerEncoder(
            (enc_attn): AnyAttention(
              (norm_q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_k): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_v): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (to_q): Linear(in_features=256, out_features=256, bias=False)
              (to_k): Linear(in_features=256, out_features=256, bias=False)
              (to_v): Linear(in_features=256, out_features=256, bias=False)
              (proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (reason): SimpleReasoning(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear): Conv1d(64, 64, kernel_size=(1,), stride=(1,), bias=False)
            )
            (enc_ffn): MLP(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (fc1): Linear(in_features=256, out_features=256, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=256, out_features=256, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (decoder): LPLayerDecoder(
            (attn1): AnyAttention(
              (norm_q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_k): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_v): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (to_q): Linear(in_features=256, out_features=256, bias=False)
              (to_k): Linear(in_features=256, out_features=256, bias=False)
              (to_v): Linear(in_features=256, out_features=256, bias=False)
              (proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (attn2): AnyAttention(
              (norm_q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_k): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_v): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (to_q): Linear(in_features=256, out_features=256, bias=False)
              (to_k): Linear(in_features=256, out_features=256, bias=False)
              (to_v): Linear(in_features=256, out_features=256, bias=False)
              (proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (ffn1): MLP(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (fc1): Linear(in_features=256, out_features=768, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=768, out_features=256, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ffn2): MLP(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (fc1): Linear(in_features=256, out_features=768, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=768, out_features=256, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (whole_qpos): PositionalEncoding(
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (4): LPBlock(
          (encoder): LPLayerEncoder(
            (enc_attn): AnyAttention(
              (norm_q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_k): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_v): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (to_q): Linear(in_features=256, out_features=256, bias=False)
              (to_k): Linear(in_features=256, out_features=256, bias=False)
              (to_v): Linear(in_features=256, out_features=256, bias=False)
              (proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (reason): SimpleReasoning(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear): Conv1d(64, 64, kernel_size=(1,), stride=(1,), bias=False)
            )
            (enc_ffn): MLP(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (fc1): Linear(in_features=256, out_features=256, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=256, out_features=256, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (decoder): LPLayerDecoder(
            (attn1): AnyAttention(
              (norm_q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_k): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_v): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (to_q): Linear(in_features=256, out_features=256, bias=False)
              (to_k): Linear(in_features=256, out_features=256, bias=False)
              (to_v): Linear(in_features=256, out_features=256, bias=False)
              (proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (attn2): AnyAttention(
              (norm_q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_k): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_v): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (to_q): Linear(in_features=256, out_features=256, bias=False)
              (to_k): Linear(in_features=256, out_features=256, bias=False)
              (to_v): Linear(in_features=256, out_features=256, bias=False)
              (proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (ffn1): MLP(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (fc1): Linear(in_features=256, out_features=768, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=768, out_features=256, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ffn2): MLP(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (fc1): Linear(in_features=256, out_features=768, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=768, out_features=256, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (whole_qpos): PositionalEncoding(
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (5): LPBlock(
          (encoder): LPLayerEncoder(
            (enc_attn): AnyAttention(
              (norm_q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_k): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_v): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (to_q): Linear(in_features=256, out_features=256, bias=False)
              (to_k): Linear(in_features=256, out_features=256, bias=False)
              (to_v): Linear(in_features=256, out_features=256, bias=False)
              (proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (reason): SimpleReasoning(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear): Conv1d(64, 64, kernel_size=(1,), stride=(1,), bias=False)
            )
            (enc_ffn): MLP(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (fc1): Linear(in_features=256, out_features=256, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=256, out_features=256, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (decoder): LPLayerDecoder(
            (attn1): AnyAttention(
              (norm_q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_k): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_v): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (to_q): Linear(in_features=256, out_features=256, bias=False)
              (to_k): Linear(in_features=256, out_features=256, bias=False)
              (to_v): Linear(in_features=256, out_features=256, bias=False)
              (proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (attn2): AnyAttention(
              (norm_q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_k): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_v): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (to_q): Linear(in_features=256, out_features=256, bias=False)
              (to_k): Linear(in_features=256, out_features=256, bias=False)
              (to_v): Linear(in_features=256, out_features=256, bias=False)
              (proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (ffn1): MLP(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (fc1): Linear(in_features=256, out_features=768, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=768, out_features=256, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ffn2): MLP(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (fc1): Linear(in_features=256, out_features=768, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=768, out_features=256, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (whole_qpos): PositionalEncoding(
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (last_encoder): LPLayerEncoder(
        (enc_attn): AnyAttention(
          (norm_q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm_k): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm_v): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (to_q): Linear(in_features=256, out_features=256, bias=False)
          (to_k): Linear(in_features=256, out_features=256, bias=False)
          (to_v): Linear(in_features=256, out_features=256, bias=False)
          (proj): Linear(in_features=256, out_features=256, bias=True)
        )
        (reason): SimpleReasoning(
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (linear): Conv1d(64, 64, kernel_size=(1,), stride=(1,), bias=False)
        )
      )
    )
  )
  (decoder): TransformerDecoder(
    (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
        )
        (linear1): Linear(in_features=256, out_features=1024, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=1024, out_features=256, bias=True)
        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
        )
        (linear1): Linear(in_features=256, out_features=1024, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=1024, out_features=256, bias=True)
        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
        )
        (linear1): Linear(in_features=256, out_features=1024, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=1024, out_features=256, bias=True)
        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
        )
        (linear1): Linear(in_features=256, out_features=1024, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=1024, out_features=256, bias=True)
        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
        )
        (linear1): Linear(in_features=256, out_features=1024, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=1024, out_features=256, bias=True)
        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
        )
        (linear1): Linear(in_features=256, out_features=1024, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=1024, out_features=256, bias=True)
        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (linear): Linear(in_features=256, out_features=10, bias=True)
  (softmax): LogSoftmax(dim=-1)
)
Namespace(adv_lambda=0.01, adv_lr=0.01, adv_theta=0.01, batch_size=256, cat_xm=False, checkpoint_every=4, checkpoint_path='../weights/language_parser/scan/defaults_mcd2.pt', d_model=256, dataset='scan', dim_feedforward=1024, dropout=0.1, filter=256, learning_rate=4e-05, load_weights_from=None, model_type='language_parser', n_layers=6, nhead=8, num_epochs=150, num_runs=1, out_attn_wts='train_defaults_mcd2_attn_maps', out_data_file='train_defaults_mcd2', pos=False, record_loss_every=20, results_dir='language_parser', split='mcd2', use_adversary=False, use_xv=False)
Model size: 40992266
LanguageParser(
  (src_embedding): Embedding(17, 256)
  (trg_embedding): Embedding(10, 256)
  (positional_encoding): PositionalEncoding(
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (encoder): LPEncoder(
    (act): GELU()
    (layer_0): Stage(
      (blocks): ModuleList(
        (0): LPBlock(
          (encoder): LPLayerEncoder(
            (enc_attn): AnyAttention(
              (norm_q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_k): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_v): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (to_q): Linear(in_features=256, out_features=256, bias=False)
              (to_k): Linear(in_features=256, out_features=256, bias=False)
              (to_v): Linear(in_features=256, out_features=256, bias=False)
              (proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (reason): SimpleReasoning(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear): Conv1d(64, 64, kernel_size=(1,), stride=(1,), bias=False)
            )
            (enc_ffn): MLP(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (fc1): Linear(in_features=256, out_features=256, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=256, out_features=256, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (decoder): LPLayerDecoder(
            (attn1): AnyAttention(
              (norm_q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_k): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_v): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (to_q): Linear(in_features=256, out_features=256, bias=False)
              (to_k): Linear(in_features=256, out_features=256, bias=False)
              (to_v): Linear(in_features=256, out_features=256, bias=False)
              (proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (attn2): AnyAttention(
              (norm_q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_k): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_v): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (to_q): Linear(in_features=256, out_features=256, bias=False)
              (to_k): Linear(in_features=256, out_features=256, bias=False)
              (to_v): Linear(in_features=256, out_features=256, bias=False)
              (proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (ffn1): MLP(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (fc1): Linear(in_features=256, out_features=768, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=768, out_features=256, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ffn2): MLP(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (fc1): Linear(in_features=256, out_features=768, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=768, out_features=256, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (whole_qpos): PositionalEncoding(
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (1): LPBlock(
          (encoder): LPLayerEncoder(
            (enc_attn): AnyAttention(
              (norm_q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_k): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_v): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (to_q): Linear(in_features=256, out_features=256, bias=False)
              (to_k): Linear(in_features=256, out_features=256, bias=False)
              (to_v): Linear(in_features=256, out_features=256, bias=False)
              (proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (reason): SimpleReasoning(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear): Conv1d(64, 64, kernel_size=(1,), stride=(1,), bias=False)
            )
            (enc_ffn): MLP(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (fc1): Linear(in_features=256, out_features=256, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=256, out_features=256, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (decoder): LPLayerDecoder(
            (attn1): AnyAttention(
              (norm_q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_k): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_v): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (to_q): Linear(in_features=256, out_features=256, bias=False)
              (to_k): Linear(in_features=256, out_features=256, bias=False)
              (to_v): Linear(in_features=256, out_features=256, bias=False)
              (proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (attn2): AnyAttention(
              (norm_q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_k): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_v): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (to_q): Linear(in_features=256, out_features=256, bias=False)
              (to_k): Linear(in_features=256, out_features=256, bias=False)
              (to_v): Linear(in_features=256, out_features=256, bias=False)
              (proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (ffn1): MLP(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (fc1): Linear(in_features=256, out_features=768, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=768, out_features=256, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ffn2): MLP(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (fc1): Linear(in_features=256, out_features=768, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=768, out_features=256, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (whole_qpos): PositionalEncoding(
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (2): LPBlock(
          (encoder): LPLayerEncoder(
            (enc_attn): AnyAttention(
              (norm_q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_k): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_v): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (to_q): Linear(in_features=256, out_features=256, bias=False)
              (to_k): Linear(in_features=256, out_features=256, bias=False)
              (to_v): Linear(in_features=256, out_features=256, bias=False)
              (proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (reason): SimpleReasoning(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear): Conv1d(64, 64, kernel_size=(1,), stride=(1,), bias=False)
            )
            (enc_ffn): MLP(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (fc1): Linear(in_features=256, out_features=256, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=256, out_features=256, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (decoder): LPLayerDecoder(
            (attn1): AnyAttention(
              (norm_q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_k): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_v): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (to_q): Linear(in_features=256, out_features=256, bias=False)
              (to_k): Linear(in_features=256, out_features=256, bias=False)
              (to_v): Linear(in_features=256, out_features=256, bias=False)
              (proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (attn2): AnyAttention(
              (norm_q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_k): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_v): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (to_q): Linear(in_features=256, out_features=256, bias=False)
              (to_k): Linear(in_features=256, out_features=256, bias=False)
              (to_v): Linear(in_features=256, out_features=256, bias=False)
              (proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (ffn1): MLP(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (fc1): Linear(in_features=256, out_features=768, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=768, out_features=256, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ffn2): MLP(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (fc1): Linear(in_features=256, out_features=768, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=768, out_features=256, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (whole_qpos): PositionalEncoding(
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (3): LPBlock(
          (encoder): LPLayerEncoder(
            (enc_attn): AnyAttention(
              (norm_q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_k): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_v): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (to_q): Linear(in_features=256, out_features=256, bias=False)
              (to_k): Linear(in_features=256, out_features=256, bias=False)
              (to_v): Linear(in_features=256, out_features=256, bias=False)
              (proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (reason): SimpleReasoning(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear): Conv1d(64, 64, kernel_size=(1,), stride=(1,), bias=False)
            )
            (enc_ffn): MLP(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (fc1): Linear(in_features=256, out_features=256, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=256, out_features=256, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (decoder): LPLayerDecoder(
            (attn1): AnyAttention(
              (norm_q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_k): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_v): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (to_q): Linear(in_features=256, out_features=256, bias=False)
              (to_k): Linear(in_features=256, out_features=256, bias=False)
              (to_v): Linear(in_features=256, out_features=256, bias=False)
              (proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (attn2): AnyAttention(
              (norm_q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_k): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_v): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (to_q): Linear(in_features=256, out_features=256, bias=False)
              (to_k): Linear(in_features=256, out_features=256, bias=False)
              (to_v): Linear(in_features=256, out_features=256, bias=False)
              (proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (ffn1): MLP(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (fc1): Linear(in_features=256, out_features=768, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=768, out_features=256, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ffn2): MLP(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (fc1): Linear(in_features=256, out_features=768, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=768, out_features=256, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (whole_qpos): PositionalEncoding(
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (layer_1): Stage(
      (blocks): ModuleList(
        (0): LPBlock(
          (encoder): LPLayerEncoder(
            (enc_attn): AnyAttention(
              (norm_q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_k): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_v): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (to_q): Linear(in_features=256, out_features=256, bias=False)
              (to_k): Linear(in_features=256, out_features=256, bias=False)
              (to_v): Linear(in_features=256, out_features=256, bias=False)
              (proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (reason): SimpleReasoning(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear): Conv1d(64, 64, kernel_size=(1,), stride=(1,), bias=False)
            )
            (enc_ffn): MLP(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (fc1): Linear(in_features=256, out_features=256, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=256, out_features=256, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (decoder): LPLayerDecoder(
            (attn1): AnyAttention(
              (norm_q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_k): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_v): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (to_q): Linear(in_features=256, out_features=256, bias=False)
              (to_k): Linear(in_features=256, out_features=256, bias=False)
              (to_v): Linear(in_features=256, out_features=256, bias=False)
              (proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (attn2): AnyAttention(
              (norm_q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_k): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_v): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (to_q): Linear(in_features=256, out_features=256, bias=False)
              (to_k): Linear(in_features=256, out_features=256, bias=False)
              (to_v): Linear(in_features=256, out_features=256, bias=False)
              (proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (ffn1): MLP(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (fc1): Linear(in_features=256, out_features=768, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=768, out_features=256, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ffn2): MLP(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (fc1): Linear(in_features=256, out_features=768, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=768, out_features=256, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (whole_qpos): PositionalEncoding(
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (1): LPBlock(
          (encoder): LPLayerEncoder(
            (enc_attn): AnyAttention(
              (norm_q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_k): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_v): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (to_q): Linear(in_features=256, out_features=256, bias=False)
              (to_k): Linear(in_features=256, out_features=256, bias=False)
              (to_v): Linear(in_features=256, out_features=256, bias=False)
              (proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (reason): SimpleReasoning(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear): Conv1d(64, 64, kernel_size=(1,), stride=(1,), bias=False)
            )
            (enc_ffn): MLP(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (fc1): Linear(in_features=256, out_features=256, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=256, out_features=256, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (decoder): LPLayerDecoder(
            (attn1): AnyAttention(
              (norm_q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_k): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_v): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (to_q): Linear(in_features=256, out_features=256, bias=False)
              (to_k): Linear(in_features=256, out_features=256, bias=False)
              (to_v): Linear(in_features=256, out_features=256, bias=False)
              (proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (attn2): AnyAttention(
              (norm_q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_k): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_v): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (to_q): Linear(in_features=256, out_features=256, bias=False)
              (to_k): Linear(in_features=256, out_features=256, bias=False)
              (to_v): Linear(in_features=256, out_features=256, bias=False)
              (proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (ffn1): MLP(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (fc1): Linear(in_features=256, out_features=768, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=768, out_features=256, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ffn2): MLP(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (fc1): Linear(in_features=256, out_features=768, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=768, out_features=256, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (whole_qpos): PositionalEncoding(
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (2): LPBlock(
          (encoder): LPLayerEncoder(
            (enc_attn): AnyAttention(
              (norm_q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_k): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_v): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (to_q): Linear(in_features=256, out_features=256, bias=False)
              (to_k): Linear(in_features=256, out_features=256, bias=False)
              (to_v): Linear(in_features=256, out_features=256, bias=False)
              (proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (reason): SimpleReasoning(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear): Conv1d(64, 64, kernel_size=(1,), stride=(1,), bias=False)
            )
            (enc_ffn): MLP(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (fc1): Linear(in_features=256, out_features=256, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=256, out_features=256, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (decoder): LPLayerDecoder(
            (attn1): AnyAttention(
              (norm_q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_k): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_v): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (to_q): Linear(in_features=256, out_features=256, bias=False)
              (to_k): Linear(in_features=256, out_features=256, bias=False)
              (to_v): Linear(in_features=256, out_features=256, bias=False)
              (proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (attn2): AnyAttention(
              (norm_q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_k): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_v): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (to_q): Linear(in_features=256, out_features=256, bias=False)
              (to_k): Linear(in_features=256, out_features=256, bias=False)
              (to_v): Linear(in_features=256, out_features=256, bias=False)
              (proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (ffn1): MLP(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (fc1): Linear(in_features=256, out_features=768, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=768, out_features=256, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ffn2): MLP(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (fc1): Linear(in_features=256, out_features=768, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=768, out_features=256, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (whole_qpos): PositionalEncoding(
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (3): LPBlock(
          (encoder): LPLayerEncoder(
            (enc_attn): AnyAttention(
              (norm_q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_k): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_v): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (to_q): Linear(in_features=256, out_features=256, bias=False)
              (to_k): Linear(in_features=256, out_features=256, bias=False)
              (to_v): Linear(in_features=256, out_features=256, bias=False)
              (proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (reason): SimpleReasoning(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear): Conv1d(64, 64, kernel_size=(1,), stride=(1,), bias=False)
            )
            (enc_ffn): MLP(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (fc1): Linear(in_features=256, out_features=256, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=256, out_features=256, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (decoder): LPLayerDecoder(
            (attn1): AnyAttention(
              (norm_q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_k): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_v): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (to_q): Linear(in_features=256, out_features=256, bias=False)
              (to_k): Linear(in_features=256, out_features=256, bias=False)
              (to_v): Linear(in_features=256, out_features=256, bias=False)
              (proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (attn2): AnyAttention(
              (norm_q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_k): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_v): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (to_q): Linear(in_features=256, out_features=256, bias=False)
              (to_k): Linear(in_features=256, out_features=256, bias=False)
              (to_v): Linear(in_features=256, out_features=256, bias=False)
              (proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (ffn1): MLP(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (fc1): Linear(in_features=256, out_features=768, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=768, out_features=256, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ffn2): MLP(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (fc1): Linear(in_features=256, out_features=768, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=768, out_features=256, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (whole_qpos): PositionalEncoding(
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (layer_2): Stage(
      (blocks): ModuleList(
        (0): LPBlock(
          (encoder): LPLayerEncoder(
            (enc_attn): AnyAttention(
              (norm_q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_k): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_v): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (to_q): Linear(in_features=256, out_features=256, bias=False)
              (to_k): Linear(in_features=256, out_features=256, bias=False)
              (to_v): Linear(in_features=256, out_features=256, bias=False)
              (proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (reason): SimpleReasoning(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear): Conv1d(64, 64, kernel_size=(1,), stride=(1,), bias=False)
            )
            (enc_ffn): MLP(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (fc1): Linear(in_features=256, out_features=256, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=256, out_features=256, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (decoder): LPLayerDecoder(
            (attn1): AnyAttention(
              (norm_q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_k): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_v): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (to_q): Linear(in_features=256, out_features=256, bias=False)
              (to_k): Linear(in_features=256, out_features=256, bias=False)
              (to_v): Linear(in_features=256, out_features=256, bias=False)
              (proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (attn2): AnyAttention(
              (norm_q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_k): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_v): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (to_q): Linear(in_features=256, out_features=256, bias=False)
              (to_k): Linear(in_features=256, out_features=256, bias=False)
              (to_v): Linear(in_features=256, out_features=256, bias=False)
              (proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (ffn1): MLP(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (fc1): Linear(in_features=256, out_features=768, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=768, out_features=256, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ffn2): MLP(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (fc1): Linear(in_features=256, out_features=768, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=768, out_features=256, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (whole_qpos): PositionalEncoding(
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (1): LPBlock(
          (encoder): LPLayerEncoder(
            (enc_attn): AnyAttention(
              (norm_q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_k): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_v): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (to_q): Linear(in_features=256, out_features=256, bias=False)
              (to_k): Linear(in_features=256, out_features=256, bias=False)
              (to_v): Linear(in_features=256, out_features=256, bias=False)
              (proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (reason): SimpleReasoning(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear): Conv1d(64, 64, kernel_size=(1,), stride=(1,), bias=False)
            )
            (enc_ffn): MLP(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (fc1): Linear(in_features=256, out_features=256, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=256, out_features=256, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (decoder): LPLayerDecoder(
            (attn1): AnyAttention(
              (norm_q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_k): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_v): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (to_q): Linear(in_features=256, out_features=256, bias=False)
              (to_k): Linear(in_features=256, out_features=256, bias=False)
              (to_v): Linear(in_features=256, out_features=256, bias=False)
              (proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (attn2): AnyAttention(
              (norm_q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_k): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_v): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (to_q): Linear(in_features=256, out_features=256, bias=False)
              (to_k): Linear(in_features=256, out_features=256, bias=False)
              (to_v): Linear(in_features=256, out_features=256, bias=False)
              (proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (ffn1): MLP(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (fc1): Linear(in_features=256, out_features=768, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=768, out_features=256, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ffn2): MLP(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (fc1): Linear(in_features=256, out_features=768, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=768, out_features=256, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (whole_qpos): PositionalEncoding(
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (2): LPBlock(
          (encoder): LPLayerEncoder(
            (enc_attn): AnyAttention(
              (norm_q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_k): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_v): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (to_q): Linear(in_features=256, out_features=256, bias=False)
              (to_k): Linear(in_features=256, out_features=256, bias=False)
              (to_v): Linear(in_features=256, out_features=256, bias=False)
              (proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (reason): SimpleReasoning(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear): Conv1d(64, 64, kernel_size=(1,), stride=(1,), bias=False)
            )
            (enc_ffn): MLP(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (fc1): Linear(in_features=256, out_features=256, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=256, out_features=256, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (decoder): LPLayerDecoder(
            (attn1): AnyAttention(
              (norm_q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_k): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_v): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (to_q): Linear(in_features=256, out_features=256, bias=False)
              (to_k): Linear(in_features=256, out_features=256, bias=False)
              (to_v): Linear(in_features=256, out_features=256, bias=False)
              (proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (attn2): AnyAttention(
              (norm_q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_k): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_v): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (to_q): Linear(in_features=256, out_features=256, bias=False)
              (to_k): Linear(in_features=256, out_features=256, bias=False)
              (to_v): Linear(in_features=256, out_features=256, bias=False)
              (proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (ffn1): MLP(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (fc1): Linear(in_features=256, out_features=768, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=768, out_features=256, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ffn2): MLP(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (fc1): Linear(in_features=256, out_features=768, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=768, out_features=256, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (whole_qpos): PositionalEncoding(
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (3): LPBlock(
          (encoder): LPLayerEncoder(
            (enc_attn): AnyAttention(
              (norm_q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_k): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_v): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (to_q): Linear(in_features=256, out_features=256, bias=False)
              (to_k): Linear(in_features=256, out_features=256, bias=False)
              (to_v): Linear(in_features=256, out_features=256, bias=False)
              (proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (reason): SimpleReasoning(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear): Conv1d(64, 64, kernel_size=(1,), stride=(1,), bias=False)
            )
            (enc_ffn): MLP(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (fc1): Linear(in_features=256, out_features=256, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=256, out_features=256, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (decoder): LPLayerDecoder(
            (attn1): AnyAttention(
              (norm_q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_k): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_v): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (to_q): Linear(in_features=256, out_features=256, bias=False)
              (to_k): Linear(in_features=256, out_features=256, bias=False)
              (to_v): Linear(in_features=256, out_features=256, bias=False)
              (proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (attn2): AnyAttention(
              (norm_q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_k): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_v): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (to_q): Linear(in_features=256, out_features=256, bias=False)
              (to_k): Linear(in_features=256, out_features=256, bias=False)
              (to_v): Linear(in_features=256, out_features=256, bias=False)
              (proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (ffn1): MLP(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (fc1): Linear(in_features=256, out_features=768, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=768, out_features=256, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ffn2): MLP(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (fc1): Linear(in_features=256, out_features=768, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=768, out_features=256, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (whole_qpos): PositionalEncoding(
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (4): LPBlock(
          (encoder): LPLayerEncoder(
            (enc_attn): AnyAttention(
              (norm_q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_k): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_v): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (to_q): Linear(in_features=256, out_features=256, bias=False)
              (to_k): Linear(in_features=256, out_features=256, bias=False)
              (to_v): Linear(in_features=256, out_features=256, bias=False)
              (proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (reason): SimpleReasoning(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear): Conv1d(64, 64, kernel_size=(1,), stride=(1,), bias=False)
            )
            (enc_ffn): MLP(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (fc1): Linear(in_features=256, out_features=256, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=256, out_features=256, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (decoder): LPLayerDecoder(
            (attn1): AnyAttention(
              (norm_q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_k): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_v): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (to_q): Linear(in_features=256, out_features=256, bias=False)
              (to_k): Linear(in_features=256, out_features=256, bias=False)
              (to_v): Linear(in_features=256, out_features=256, bias=False)
              (proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (attn2): AnyAttention(
              (norm_q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_k): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_v): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (to_q): Linear(in_features=256, out_features=256, bias=False)
              (to_k): Linear(in_features=256, out_features=256, bias=False)
              (to_v): Linear(in_features=256, out_features=256, bias=False)
              (proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (ffn1): MLP(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (fc1): Linear(in_features=256, out_features=768, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=768, out_features=256, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ffn2): MLP(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (fc1): Linear(in_features=256, out_features=768, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=768, out_features=256, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (whole_qpos): PositionalEncoding(
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (5): LPBlock(
          (encoder): LPLayerEncoder(
            (enc_attn): AnyAttention(
              (norm_q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_k): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_v): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (to_q): Linear(in_features=256, out_features=256, bias=False)
              (to_k): Linear(in_features=256, out_features=256, bias=False)
              (to_v): Linear(in_features=256, out_features=256, bias=False)
              (proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (reason): SimpleReasoning(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear): Conv1d(64, 64, kernel_size=(1,), stride=(1,), bias=False)
            )
            (enc_ffn): MLP(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (fc1): Linear(in_features=256, out_features=256, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=256, out_features=256, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (decoder): LPLayerDecoder(
            (attn1): AnyAttention(
              (norm_q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_k): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_v): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (to_q): Linear(in_features=256, out_features=256, bias=False)
              (to_k): Linear(in_features=256, out_features=256, bias=False)
              (to_v): Linear(in_features=256, out_features=256, bias=False)
              (proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (attn2): AnyAttention(
              (norm_q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_k): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_v): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (to_q): Linear(in_features=256, out_features=256, bias=False)
              (to_k): Linear(in_features=256, out_features=256, bias=False)
              (to_v): Linear(in_features=256, out_features=256, bias=False)
              (proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (ffn1): MLP(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (fc1): Linear(in_features=256, out_features=768, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=768, out_features=256, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ffn2): MLP(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (fc1): Linear(in_features=256, out_features=768, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=768, out_features=256, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (whole_qpos): PositionalEncoding(
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (layer_3): Stage(
      (blocks): ModuleList(
        (0): LPBlock(
          (encoder): LPLayerEncoder(
            (enc_attn): AnyAttention(
              (norm_q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_k): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_v): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (to_q): Linear(in_features=256, out_features=256, bias=False)
              (to_k): Linear(in_features=256, out_features=256, bias=False)
              (to_v): Linear(in_features=256, out_features=256, bias=False)
              (proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (reason): SimpleReasoning(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear): Conv1d(64, 64, kernel_size=(1,), stride=(1,), bias=False)
            )
            (enc_ffn): MLP(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (fc1): Linear(in_features=256, out_features=256, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=256, out_features=256, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (decoder): LPLayerDecoder(
            (attn1): AnyAttention(
              (norm_q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_k): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_v): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (to_q): Linear(in_features=256, out_features=256, bias=False)
              (to_k): Linear(in_features=256, out_features=256, bias=False)
              (to_v): Linear(in_features=256, out_features=256, bias=False)
              (proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (attn2): AnyAttention(
              (norm_q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_k): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_v): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (to_q): Linear(in_features=256, out_features=256, bias=False)
              (to_k): Linear(in_features=256, out_features=256, bias=False)
              (to_v): Linear(in_features=256, out_features=256, bias=False)
              (proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (ffn1): MLP(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (fc1): Linear(in_features=256, out_features=768, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=768, out_features=256, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ffn2): MLP(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (fc1): Linear(in_features=256, out_features=768, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=768, out_features=256, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (whole_qpos): PositionalEncoding(
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (1): LPBlock(
          (encoder): LPLayerEncoder(
            (enc_attn): AnyAttention(
              (norm_q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_k): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_v): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (to_q): Linear(in_features=256, out_features=256, bias=False)
              (to_k): Linear(in_features=256, out_features=256, bias=False)
              (to_v): Linear(in_features=256, out_features=256, bias=False)
              (proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (reason): SimpleReasoning(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear): Conv1d(64, 64, kernel_size=(1,), stride=(1,), bias=False)
            )
            (enc_ffn): MLP(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (fc1): Linear(in_features=256, out_features=256, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=256, out_features=256, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (decoder): LPLayerDecoder(
            (attn1): AnyAttention(
              (norm_q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_k): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_v): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (to_q): Linear(in_features=256, out_features=256, bias=False)
              (to_k): Linear(in_features=256, out_features=256, bias=False)
              (to_v): Linear(in_features=256, out_features=256, bias=False)
              (proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (attn2): AnyAttention(
              (norm_q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_k): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_v): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (to_q): Linear(in_features=256, out_features=256, bias=False)
              (to_k): Linear(in_features=256, out_features=256, bias=False)
              (to_v): Linear(in_features=256, out_features=256, bias=False)
              (proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (ffn1): MLP(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (fc1): Linear(in_features=256, out_features=768, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=768, out_features=256, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ffn2): MLP(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (fc1): Linear(in_features=256, out_features=768, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=768, out_features=256, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (whole_qpos): PositionalEncoding(
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (2): LPBlock(
          (encoder): LPLayerEncoder(
            (enc_attn): AnyAttention(
              (norm_q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_k): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_v): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (to_q): Linear(in_features=256, out_features=256, bias=False)
              (to_k): Linear(in_features=256, out_features=256, bias=False)
              (to_v): Linear(in_features=256, out_features=256, bias=False)
              (proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (reason): SimpleReasoning(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear): Conv1d(64, 64, kernel_size=(1,), stride=(1,), bias=False)
            )
            (enc_ffn): MLP(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (fc1): Linear(in_features=256, out_features=256, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=256, out_features=256, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (decoder): LPLayerDecoder(
            (attn1): AnyAttention(
              (norm_q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_k): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_v): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (to_q): Linear(in_features=256, out_features=256, bias=False)
              (to_k): Linear(in_features=256, out_features=256, bias=False)
              (to_v): Linear(in_features=256, out_features=256, bias=False)
              (proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (attn2): AnyAttention(
              (norm_q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_k): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_v): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (to_q): Linear(in_features=256, out_features=256, bias=False)
              (to_k): Linear(in_features=256, out_features=256, bias=False)
              (to_v): Linear(in_features=256, out_features=256, bias=False)
              (proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (ffn1): MLP(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (fc1): Linear(in_features=256, out_features=768, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=768, out_features=256, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ffn2): MLP(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (fc1): Linear(in_features=256, out_features=768, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=768, out_features=256, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (whole_qpos): PositionalEncoding(
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (3): LPBlock(
          (encoder): LPLayerEncoder(
            (enc_attn): AnyAttention(
              (norm_q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_k): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_v): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (to_q): Linear(in_features=256, out_features=256, bias=False)
              (to_k): Linear(in_features=256, out_features=256, bias=False)
              (to_v): Linear(in_features=256, out_features=256, bias=False)
              (proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (reason): SimpleReasoning(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear): Conv1d(64, 64, kernel_size=(1,), stride=(1,), bias=False)
            )
            (enc_ffn): MLP(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (fc1): Linear(in_features=256, out_features=256, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=256, out_features=256, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (decoder): LPLayerDecoder(
            (attn1): AnyAttention(
              (norm_q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_k): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_v): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (to_q): Linear(in_features=256, out_features=256, bias=False)
              (to_k): Linear(in_features=256, out_features=256, bias=False)
              (to_v): Linear(in_features=256, out_features=256, bias=False)
              (proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (attn2): AnyAttention(
              (norm_q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_k): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_v): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (to_q): Linear(in_features=256, out_features=256, bias=False)
              (to_k): Linear(in_features=256, out_features=256, bias=False)
              (to_v): Linear(in_features=256, out_features=256, bias=False)
              (proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (ffn1): MLP(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (fc1): Linear(in_features=256, out_features=768, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=768, out_features=256, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ffn2): MLP(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (fc1): Linear(in_features=256, out_features=768, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=768, out_features=256, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (whole_qpos): PositionalEncoding(
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (4): LPBlock(
          (encoder): LPLayerEncoder(
            (enc_attn): AnyAttention(
              (norm_q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_k): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_v): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (to_q): Linear(in_features=256, out_features=256, bias=False)
              (to_k): Linear(in_features=256, out_features=256, bias=False)
              (to_v): Linear(in_features=256, out_features=256, bias=False)
              (proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (reason): SimpleReasoning(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear): Conv1d(64, 64, kernel_size=(1,), stride=(1,), bias=False)
            )
            (enc_ffn): MLP(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (fc1): Linear(in_features=256, out_features=256, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=256, out_features=256, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (decoder): LPLayerDecoder(
            (attn1): AnyAttention(
              (norm_q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_k): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_v): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (to_q): Linear(in_features=256, out_features=256, bias=False)
              (to_k): Linear(in_features=256, out_features=256, bias=False)
              (to_v): Linear(in_features=256, out_features=256, bias=False)
              (proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (attn2): AnyAttention(
              (norm_q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_k): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_v): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (to_q): Linear(in_features=256, out_features=256, bias=False)
              (to_k): Linear(in_features=256, out_features=256, bias=False)
              (to_v): Linear(in_features=256, out_features=256, bias=False)
              (proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (ffn1): MLP(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (fc1): Linear(in_features=256, out_features=768, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=768, out_features=256, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ffn2): MLP(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (fc1): Linear(in_features=256, out_features=768, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=768, out_features=256, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (whole_qpos): PositionalEncoding(
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (5): LPBlock(
          (encoder): LPLayerEncoder(
            (enc_attn): AnyAttention(
              (norm_q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_k): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_v): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (to_q): Linear(in_features=256, out_features=256, bias=False)
              (to_k): Linear(in_features=256, out_features=256, bias=False)
              (to_v): Linear(in_features=256, out_features=256, bias=False)
              (proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (reason): SimpleReasoning(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear): Conv1d(64, 64, kernel_size=(1,), stride=(1,), bias=False)
            )
            (enc_ffn): MLP(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (fc1): Linear(in_features=256, out_features=256, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=256, out_features=256, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (decoder): LPLayerDecoder(
            (attn1): AnyAttention(
              (norm_q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_k): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_v): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (to_q): Linear(in_features=256, out_features=256, bias=False)
              (to_k): Linear(in_features=256, out_features=256, bias=False)
              (to_v): Linear(in_features=256, out_features=256, bias=False)
              (proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (attn2): AnyAttention(
              (norm_q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_k): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (norm_v): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (to_q): Linear(in_features=256, out_features=256, bias=False)
              (to_k): Linear(in_features=256, out_features=256, bias=False)
              (to_v): Linear(in_features=256, out_features=256, bias=False)
              (proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (ffn1): MLP(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (fc1): Linear(in_features=256, out_features=768, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=768, out_features=256, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (ffn2): MLP(
              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (fc1): Linear(in_features=256, out_features=768, bias=True)
              (act): GELU()
              (fc2): Linear(in_features=768, out_features=256, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (whole_qpos): PositionalEncoding(
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (last_encoder): LPLayerEncoder(
        (enc_attn): AnyAttention(
          (norm_q): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm_k): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm_v): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (to_q): Linear(in_features=256, out_features=256, bias=False)
          (to_k): Linear(in_features=256, out_features=256, bias=False)
          (to_v): Linear(in_features=256, out_features=256, bias=False)
          (proj): Linear(in_features=256, out_features=256, bias=True)
        )
        (reason): SimpleReasoning(
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (linear): Conv1d(64, 64, kernel_size=(1,), stride=(1,), bias=False)
        )
      )
    )
  )
  (decoder): TransformerDecoder(
    (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
        )
        (linear1): Linear(in_features=256, out_features=1024, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=1024, out_features=256, bias=True)
        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
        )
        (linear1): Linear(in_features=256, out_features=1024, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=1024, out_features=256, bias=True)
        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
        )
        (linear1): Linear(in_features=256, out_features=1024, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=1024, out_features=256, bias=True)
        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
        )
        (linear1): Linear(in_features=256, out_features=1024, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=1024, out_features=256, bias=True)
        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (4): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
        )
        (linear1): Linear(in_features=256, out_features=1024, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=1024, out_features=256, bias=True)
        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
      (5): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
        )
        (linear1): Linear(in_features=256, out_features=1024, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=1024, out_features=256, bias=True)
        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (linear): Linear(in_features=256, out_features=10, bias=True)
  (softmax): LogSoftmax(dim=-1)
)